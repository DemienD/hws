# -*- coding: utf-8 -*-
"""[Студену] ДЗ 14 Базовое.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-3TQo0GtIaivqe2LcvZ3nW6j3tB4wWhG



# Домашнее задание на тему «Этика ИИ»
Обмани картиночную нейросеть двумя способами:

1) измени картинку целиком, но так, чтобы это было незаметно для глаз;

2) замени малую часть картинки на патч.

## Подготовка

Установим пакеты, необходимые для визуализации предсказаний нейросети.
"""

# Пригодится для 1-го задания. Если делаешь 2-е, то можешь не запускать эту ячейку
!pip install grad-cam captum --quiet

"""В следующей ячейке установим NumPy 1.26.4 и перезапустим среду.

Текущая среда отключится, а новая, с библиотекой Numpy 1.26.4, подключится. Все загруженные пакеты сохранятся. Отключение и перезапуск среды нужны для того, чтобы импортировалась правильная версия NumPy, а с версией 2.x.x пакеты для визуализации работать не будут.

Имей в виду, что из-за этой ячейки не получится сделать `Run all`: команда не выполнится, потому что среда перезапустится.
"""

# Установим более старую версию NumPy. С версией NumPy 2.x.x модель не работает
!pip install "numpy==1.26.4" -- quiet

# Перезапустим среду, чтобы можно было использовать новые версии пакетов. Все файлы сохранятся
import os
os.kill(os.getpid(), 9)

"""## Задание 1. Обман модели одной картинкой (5 баллов)
Мы поработаем с нейросетью, предобученной на датасете Imagenet 1k. Этот датасет используется в задачах классификации и состоит из различных изображений, относящихся к 1000 классов.

Твоя задача — обмануть нейросеть, изменив всю картинку.

1. Создай функцию, которая приводит изображение к стандартному виду. (1 балл)
2. Сделай функцию, с помощью которой можно получить top-k наиболее вероятных классов для изображения и вероятности этих классов. (1 балл)
3. Измени веса изображения градиентным методом. (3 балла)

Загрузим json "индекс класса: название класса". Он понадобится для интерпретации ответов нейросети.
"""

!wget -q 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json' -O imagenet_class_index.json

# Импортируй пакеты, которые пригодятся для задания 1

import os
import json

import numpy as np

from matplotlib import pylab as P
import matplotlib.pyplot as plt
from PIL import Image

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.autograd import Variable
import torch.nn.functional as F
from torchvision import transforms, models

from captum.attr import IntegratedGradients

# Определяем, что хотим использовать видеокарту, если она доступна
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Загружаем предобученную модель c весами для Imagenet 1K. Датасет для классификации на 1000 классов
model = models.resnet18(weights="ResNet18_Weights.DEFAULT") # https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html

# Распаковываем json и форматируем для удобства работы. Пригодится для получения баллов в ДЗ
idx2label, cls2label, cls2idx = [], {}, {}
with open(os.path.abspath("/content/imagenet_class_index.json"), "r") as read_file:
    class_idx = json.load(read_file)
    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]
    lable2idx = {class_idx[str(k)][1]: k for k in range(len(class_idx))}

"""У нас простая свёрточная модель, которая через 2 свёрточных блока производит сжатие картинки в 2 раза. Также в обход этих блоков прокидывается дополнительная связь, и благодаря этому градиент не затухает.
То есть: `x = x + conv_block(x)`.
"""

model

"""Модель — это классический Resnet из 4 больших блоков. После блоков вся картинка усредняется в один вектор, а потом этот вектор приводится к размерности 1000, потому что в Imagenet 1k — 1000 классов.

Поработаем с картинкой сибирского кота из «Википедии». Вообще можно взять любую другую картинку, но важно, чтобы она была похожа на изображения из ImageNet 1k, где на переднем плане в центре находится один-единственный объект.
"""

# Загружаем картинку и рисуем
!wget 'https://upload.wikimedia.org/wikipedia/commons/0/06/Sibirische-Katze-Omega-1.jpg'
path = '/content/Sibirische-Katze-Omega-1.jpg'

def get_image(path):
    with open(os.path.abspath(path), "rb") as f:
        with Image.open(f) as img:
            return img.convert("RGB")


img = get_image(path)
plt.rcParams["figure.figsize"] = (5, 5)
plt.imshow(img)
plt.show()

"""### 1. Задай параметры трансформации картинки в формат Imagenet 1K (1 балл)

Imagenet 1K — это стандартный датасет, на котором уже предобучена модель. Чтобы модель корректно работала на выбранном изображении, изображение должно соответствовать датасету.

Обратись к [документации](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18) torchvision и посмотри, какие преобразования применялись к датасету для обучения модели.

Напиши функцию, которая последовательно применяет эти преобразования. Не забудь применить преобразование `ToTensor` (модель работает с тензорами).

#### Твоё решение
"""

def get_input_transform():
    transform = transforms.Compose([
        # Все параметры преобразований выбраны такими, так как именно с ними модель училась на датасете Imagenet_1k
        transforms. # Увеличь масштаб изображения
        transforms. # Вырежь серединку
        transforms. # Преобразуй в тензор
        transforms. # Нормализуй
    ])
    return transform

# Обратное преобразование пригодится для дальнейших визуализаций
def get_reverse_transform():
    transform = transforms.Compose([
        # Обратное преобразование к нормализации. Разделили на две части, чтобы написать через transforms
        transforms.Normalize(
            mean=(0., 0., 0.),
            std=(1/0.229, 1/0.224, 1/0.225)
        ),
        transforms.Normalize(
            mean=(-0.485, -0.456, -0.406),
            std=(1., 1., 1.),
        ),
        # Переставляем каналы в конец
        transforms.Lambda(
            lambda x: torch.permute(x, (0, 2, 3, 1))
        ),
        # Отключаем градиент, сжимаем размерность батча и переводим в numpy
        transforms.Lambda(
            lambda x: x.detach().squeeze().numpy()
        ),
    ])
    return transform

# Функция, чтобы получить итоговый тензор, с которым мы будем работать, из входного изображения
def get_input_tensors(img):
    transform = get_input_transform()
    # Делаем трансформацию и переводим к размерности батча
    return transform(img).unsqueeze(0)

# Функция, чтобы получить изображение из тензора
def get_crop_img(img_tensor):
    transform = get_reverse_transform()
    return transform(img_tensor)

# Получаем тензор из исходного изображения
img_t = get_input_tensors(img)

# Проверяем работу функции для обратного преобразования. Если преобразования правильно написаны, то должно получиться немного обрезанное исходное изображение
plt.imshow(get_crop_img(img_t))
plt.show()

"""### 2. Напиши функцию, чтобы выводить top-k наиболее вероятных классов для изображения и их вероятности (1 балл)

#### Твоё решение
"""

# Функция, чтобы из логитов получить распределение вероятностей и напечатать их с соответствующими индексами и классами
def top_k_class(model, img, k=6):
    model.to(device)
    model.eval()
    logits = Получи логиты (выход последнего слоя нейросети) для изображения
    prediction = Через softmax преобразуй логиты к вероятностям и сожми по размерности батча
    top_props, top_inds = prediction.topk(k) # Получи топ значений и их индексы
    for i in range(k):
        category_name = Достань индекс i-го объекта из топа и верни соответствующее текстовое название
        score = Достань вероятность для i-го объекта из топа
        print(f"{category_name} {Достань индекс i-го объекта из топа}: {100 * score:.1f}%")

top_k_class(model, img_t)

"""Хотя в Imagenet_1k 1000 классов и полосок у кота немного, модель предсказывает, что на изображении полосатый кот (tabby) с вероятностью чуть больше 50%, что очень далеко от случайного предсказания.

В топе выдач можно наблюдать породы кошек, но в датасете нет отдельного класса для сибирской породы.

Визуализируем то, на что ориентируется модель при выборе класса tabby. Начнём с Grad Cam.
"""

from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget

target_layers = [model._modules["layer4"]] # Берём последний свёрточный блок для визуализации, так как там собраны наиболее верхнеуровневые фичи

cam = GradCAM(model=model, target_layers=target_layers) # Инициализируем наш класс
target = [ClassifierOutputTarget([281])] # Используем 281 логит (класс tabby) как целевой
cam_map = cam(input_tensor=img_t, targets=target)

"""Также сделаем визуализацию методом интегрированных градиентов."""

integrated_gradients = IntegratedGradients(model)
attributions_ig = integrated_gradients.attribute(
    img_t.to(device),
    target=283,
    n_steps=200
)

# Функция для визуализации градиентов для IntegratedGradients
def grad_to_image(raw_grads, percentile=99):
    # Получим градиенты
    gradients = raw_grads.detach().cpu().numpy()
    # Преобразуем их к виду изображения
    gradients = np.transpose(gradients, (0, 2, 3, 1))[0]
    image_2d = np.sum(np.abs(gradients), axis=2)
    # Вычисляем параметры для нормализации
    vmax = np.percentile(image_2d, percentile)
    vmin = np.min(image_2d)
    # На выходе аналог min-max нормализации, но только по перцентилям
    return np.clip((image_2d - vmin) / (vmax - vmin), 0, 1)

# Получаем карту внимания для входного изображения
saliency_map = grad_to_image(attributions_ig)

# Функция для отрисовки карты внимания для IntegratedGradients в тонах серого (так красивее)
def ShowGrayscaleImage(im, title='', ax=None):
    if ax is None:
        P.figure()
    P.axis('off')
    P.imshow(im, cmap=P.cm.gray, vmin=0, vmax=1)
    P.title(title)

# Рисуем то, что у нас получилось

plt.rcParams["figure.figsize"] = (20, 20)

# Исходное изображение
plt.subplot(3, 3, 1)
plt.imshow(get_crop_img(img_t))
plt.title('Original')
plt.axis('off')

# GradCam
plt.subplot(3, 3, 2)
plt.imshow(get_crop_img(img_t))
plt.imshow(cam_map[0], alpha=0.6, interpolation="bilinear", cmap="jet")
plt.title('Original Grad Cam')

# IntegratedGradients
ShowGrayscaleImage(
    saliency_map,
    title='Original Integrated Gradients',
    ax=P.subplot(3, 3, 3)
)

"""Модели нужно посмотреть на кота, чтобы сказать, что на картинке кот.

### 3. Измени изображение градиентными методами. FGM Adversarial Attack  (3 балла)

Обманем нейросеть для классификации картинок с помощью метода FGM (Fast Gradient Method).

Вероятность того, что на картинке посудомоечная машина (dishwasher), составила 0,9%. Попробуем сделать так, чтобы эта вероятность возросла. Мы  обманем нейросеть: изменим изображение градиентными методами так, что для человеческого восприятия ничего не изменится.

Изображение — это точно такой же тензор, как и параметры нейросети. Если мы заморозим всю сеть (уже заморозили, когда сделали  .eval() выше) и разморозим параметры (пиксели) самого изображения, то сможем изменять его при обучении. То есть мы будем идти против градиента функции потерь и изменять с помощью оптимизатора только пиксели самого изображения.

Для этого необходимо реализовать стандартный loop обучения, но в качестве функции потерь будет выступать вероятность необходимого класса. Для посудомоечной машины номер такого класса — 534.

#### Твоё решение
"""

# Делаем копию исходного изображения и задаём, что мы хотим, чтобы оно было подвержено изменениям градиентными методами
img_dish = Variable(img_t.clone(), requires_grad=True)
# Инициализируем оптимизатор, передаём ему обучаемое изображение в качестве параметров и говорим, что хотим использовать для максимизации (так как будем использовать вероятность в качестве loss, которую мы хотим сделать максимальной)
optimizer = Adam([img_dish], maximize=True)

# Пока вероятность не достигнет желаемой, обучаем изображение
while True:
  # Пропускаем изображение через нейросеть (получаем логиты), переводим их к вероятностному представлению и выбираем логит, который относится к посудомоечной машинке. Он под индексом 534. Это указано в словаре, который загружали в начале
  loss = Твой код
  print(loss)
  # Хотим учиться, пока вероятность предсказания посудомоечной машины не станет более 98%
  Твой код
  # Обратное распространение ошибки
  loss.backward()
  # Двигаем веса
  optimizer.step()
  # Обнуляем значения градиентов
  optimizer.zero_grad()

"""Проделываем все те же самые действия для визуализации интерпретаций предсказания, но уже для изменённого изображения."""

attributions_curt = integrated_gradients.attribute(
    img_dish.to(device),
    target=534,
    n_steps=200
)

saliency_dish = grad_to_image(img_dish)
target_dish = [ClassifierOutputTarget([534])]
cam_dish = cam(input_tensor=img_dish, targets=target_dish)

"""Визуализируем в сравнении с оригиналом."""

plt.rcParams["figure.figsize"] = (20, 20)

# Исходное изображение
plt.subplot(3, 3, 1)
plt.imshow(get_crop_img(img_t))
plt.title('Original')
plt.axis('off')

# GradCam для исходного изображения
plt.subplot(3, 3, 2)
plt.imshow(get_crop_img(img_t))
plt.imshow(cam_map[0], alpha=0.6, interpolation="bilinear", cmap="jet")
plt.title('Original Grad Cam')

# IntegratedGradients для исходного изображения
ShowGrayscaleImage(
    saliency_map,
    title='Original Integrated Gradients',
    ax=P.subplot(3, 3, 3)
)

# Изменённное изображение
plt.subplot(3, 3, 4)
plt.imshow(get_crop_img(img_dish))
plt.title('Dishwasher')
plt.axis('off')

# GradCam для изменённого изображения
plt.subplot(3, 3, 5)
plt.imshow(get_crop_img(img_dish))
plt.imshow(cam_dish[0], alpha=0.6, interpolation="bilinear", cmap="jet")
plt.title('Dishwasher Grad Cam')

# IntegratedGradients для изменённого изображения
ShowGrayscaleImage(
    saliency_dish,
    title='Dishwasher Integrated Gradients',
    ax=P.subplot(3, 3, 6)
)

"""Теперь нейросеть считает, что мы подали ей на вход посудомоечную машину, а не кота. Для человеческого восприятия изображение не изменилось.

Если проводить визуализации методами Grad Cam и Integrated Gradients, то явно видны отличия в восприятии моделью входа. То есть, казалось бы, при одинаковых (на взгляд человека) картинках на входе модель по-разному на них реагирует.

Чтобы убедиться, что картинки действительно различаются, можно посмотреть на тензоры: значения в них разные.
"""

img_t, img_dish

"""Значения чуть-чуть разные, именно поэтому мы не видим отличий, но для нейросети изображение изменилось кардинально.

Убедимся, что модель воспринимает обновлённое изображение как посудомоечную машину.
"""

top_k_class(model, img_dish)

"""## Задание 2. Adversatial patch (5 баллов)

Только что мы обучали изображение: чтобы изображение отнести к какому-то классу, необходимо целиком его изменять, но на это не всегда есть ресурсы. В идеале хотелось бы обучить что-то один раз, а потом уже это использовать.

Можно обучить небольшой патч, который будет прикрепляться к любой картинке в датасете и менять вероятности предсказания объектов на ней. Обучим такие патчи и оценим качество их обмана в зависимости от размера патча.

Мы будем работать с 10 классами из ImageNet 1K, потому что целиком ImageNet 1K очень большой для  задания (и для Colab).

В задании тебе нужно:

1) реализовать класс Dataset, который будет подгружать изображения с диска (1 балл);

2) определить класс нейросети с обучаемым патчем (2 балла);

3) изменить обучающий loop под данную задачу (1 балл);

4) оценить качество обучения для разных размеров патча (1 балл).

Загрузи необходимые для работы данные.
"""

# Скачаем train- и val-часть нашего датасета
!gdown 1QActyStoajNtF1QQlguLj2w_9Tm9Pc3k
!gdown 18wgvX3GtIbHS_zuVQ2SuVCDABbkMFXPL

# И распакуем
!unzip -qq /content/data_train.zip -d /content/data_train
!unzip -qq /content/data_test.zip -d /content/data_test

import os
from tqdm import tqdm

import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.optim import Adam
import torch.nn.functional as F
from torchvision import transforms, models

from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

"""#### Определи преобразования для датасета (скопируй из задания 1, если оно у тебя сделано)"""

# Определим преобразования для ImageNet (они стандартные, просто скопируй из прошлого задания)
# Преобразования разбиваем на две части для удобства, но по сути это те же самые преобразования, что у тебя определены выше. Поэтому просто скопируй
# Эти преобразования будем применять во время считывания изображения, чтобы они были все одинаковой размерности
preprocess = transforms.Compose([
        transforms.Увеличь масштаб изображения,
        transforms.Вырежь серединку
        ])

# А эти будем применять на этапе сбора батча изображения
base = transforms.Compose([
        transforms.Преобразуй в тензор,
        transforms.Нормализуй
])

"""#### 1. Реализуй класс Dataset, который будет подгружать изображения с диска (1 балл)

#### Твоё решение
"""

# Определим класс датасета
class ImageNet10(Dataset):

  # В init всё будем считывать с диска
  def __init__(self, root, transforms_preprocess=None, transforms_base=None):
    '''
    root — путь до папки, откуда считываем
    transforms_preprocess — трансформации, которые применияем в процессе считывания
    transforms_base — трансформации, которые применяем в процессе сбора батча
    '''
    self.X = [] # Контейнер, куда складываем изображения
    self.y = [] # Контейнер, куда складываем метки

    folders = sorted(os.listdir(f'/content/{root}')) # Сортировка нужна, потому что unzip работает параллельно, и может получиться, что неверно соотнесём классы и объекты, если не отсортировать
    # Итерируемся по папкам
    for i, folder in enumerate(folders):
      files = os.listdir(f'/content/{root}/{folder}')
      # Добавляем в self.y метки класса (i) по количеству файлов в папке
      self.y.extend(Твой код)
      # Итерируемся по файлам в каждой папке
      for file in tqdm(files):
        # Считываем изображение, преобразуем к стандартному формату и кладём в контейнер
        self.X.append(Твой код, путь до файла уже верный указанf'/content/{root}/{folder}/{file}')

    self.y = np.array(self.y) # Преобразуем self.y в np.array
    self.transforms = transforms_base # Сохраняем базовую трансформацию в классе

    # В данных есть следующие классы. Поместим информацию о них в ImageNet10: так удобнее, потому что всё будет находиться внутри одного объекта
    self.classes = {0: 'hen',
              1: 'kite',
              2: 'terrapin',
              3: 'scorpion',
              4: 'centipede',
              5: 'boxer',
              6: 'tabby',
              7: 'ice bear',
              8: 'hare',
              9: 'broom'}

  def __len__(self):
    return len(self.y)

  # Если определена трансформация, применяем её, иначе возвращаем без трансформаций
  def __getitem__(self, idx):
    Твой код

# Опередели датасеты
train_set = ImageNet10('data_train', preprocess, base)
test_set = ImageNet10('data_test', preprocess, base)
# Это пригодится для вычисления метрик
y_test = test_set.y

train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
test_loader = DataLoader(test_set, batch_size=8, shuffle=False) # Батч сайз такой маленький, потому что будем в случайном месте прикреплять патч в рамках одного батча, тем меньше батч на валидации, тем стабильнее оценка будет

for X, y in train_loader:
  break

X.shape, y.shape

"""Сначала определим функции, которые будем использовать для проверки качества обмана. Обмануть можно двумя способами:

1) модель будет выдавать для изображения с патчем класс, патч для которого мы и выучили;

2) благодаря патчу можно ошибаться в предсказании истинного класса. То есть модель будет предсказывать метку класса, которая не соответствует патчу, но при этом будет ошибаться с предсказанием истинного класса.
"""

# Общую точность модели будем смотреть через обычный accuracy
from sklearn.metrics import accuracy_score

# А ошибку в предсказании истинного класса будем измерять как долю всех объектов, которые верно предсказали классом для обмана
def accuracy_score_cls(y_pred, cls):
  '''
  y_pred — предсказанные метки
  cls — индекс класса, которым мы пытаемся обмануть
  '''
  return (y_pred == cls).mean()

"""Посмотрим, какие значения метрик будет выдавать модель, которую заранее обучили на CIFAR-10."""

# Можно почитать, что здесь происходит, но это не особо важно. Просто функция для отрисовки изображений
def show_ds(ds, n=0): # Принимает датасет и порядковый номер изображения из класса
    # Хотим получить по изображению каждого класса
    fig, axs = plt.subplots(1, len(ds.classes), figsize=(20, 5))
    # Итерируемся по каждому классу
    for cls_num, name in ds.classes.items():
        # Берём n-е изображение класса в датасете
        i = np.argwhere(ds.y == cls_num)[n][0]
        # Рисуем изображение
        ax = axs[cls_num]
        ax.imshow(ds.X[i], cmap="Greys_r")
        ax.set_title(name)
        ax.axis("off")
    plt.show()

show_ds(train_set, 1)

"""### 2–3. Напиши класс нейросети для обучения патча и измени стандарный loop для обучения (2 + 1 балл)

Загрузим модель, которая уже обучена специально под наши 10 классов, и проверим, как она работает.
"""

!gdown 1yShvYAlm8EjJVoui6l6XYbwgQzjO0TB5

"""### Инициализируй backbone модели для обучения патча

#### Твоё решение
"""

# Определим device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# Определим модель
model_cls = models.mobilenet_v3_small()
# Меняем последний слой на классификацию под 10 классов из Imagenet 1K. Посмотри, как выглядит нейросеть, чтобы задать нужное количество входов и выходов в последнем блоке
model_cls.classifier[3] = Твой код
# Подгружаем веса модели
model_cls.load_state_dict(torch.load('/content/model_epoch_2.pth', weights_only=True, map_location=device))
model_cls.to(device).eval()

"""Определим функции для получения предсказаний. Они же будут использоваться при обучении на этапе валидации, отсюда и название. И это снова функции из стандарного loop для обучения с небольшими изменениями под задачу.

### Определи функции для валидации/получения предсказаний

#### Твой код
"""

# Функция для одного шага валидации
# Теперь вместо функции потерь передаём label_cls — это индекс класса, которым хотим обмануть модель
def valid_step(batch, model, label_cls, device):
      X, _ = batch
      X = X.to(device)

      with torch.no_grad():
        logits = model(X) # И для удобства модель будет возвращать кое-что ещё, посмотрим на это чуть ниже. Для валидации нам это не надо
        softmax = Получи распределение вероятностей для каждого объекта в батче
        l = Твой код # Изменение здесь — функция потерь. Мы считаем её как вероятность нужного класса, которым обманываем нейросеть. То есть достаём из батча вероятности нужного класса для каждого объекта и усредняем
        preds = Предсказание — наиболее вероятный класс (индекс) из softmax

      return l.item(), preds.cpu().numpy()

# Тут, кроме label_cls в аргументах, — без изменений
def validate(model, label_cls, device, val_dataloader):
  val_loss = 0
  preds = []
  for batch in tqdm(val_dataloader):
    loss_step, preds_step = valid_step(batch, model, label_cls, device)

    val_loss += loss_step / len(val_dataloader)
    preds.append(preds_step)
  preds = np.concatenate(preds)

  return val_loss, preds

"""Получим предсказания и посчитаем на них заданные метрики."""

# Индекс класса зададим 6 (кот/tabby)
_, preds = validate(model_cls, 6, device, test_loader)

# Считаем метрики
acc = round(accuracy_score(y_test, preds) * 100, 2)
acc_cls = round(accuracy_score_cls(preds, 6) * 100, 2)

print()
print(f'Val acc {acc}% | Val acc cls {acc_cls}%')

"""Изначально модель неплохо справляется с классификацией всех изображений — лучше случайного качества (10%). А для котов качество получается около 10,2%, потому что точность модели составляет 89%, и датасет сбалансированный, то есть в нём 10% объектов tabby. Модель может отклоняться в каком-то интервале от идеальных 10%, так как допускает ошибки.

Теперь перейдём к построению модели для обучения патча для обмана.

Определим класс нашей нейросети, которая будет вставлять патч в каждое изображение.

Посмотрим, что для этого понадобится.
1. Загрузить модель MobileNet_v3_small (так как мы будем обучаться на всех изображениях в датасете, нужна быстрая сеть) с соответствующими CIFAR-10 весами.
2. Инициализировать из равномерного шума обучаемый патч с помощью `torch.rand` и `nn.Parameter`.
3. Во время прямого прохода по сети:
  * нормализовать патч с теми же параметрами, что и изображения;
  * сгенерировать два целых числа из диапазона `(0, размер изображения — размер патча)`, они будут обозначать место на картинке, куда прикреплять патч.
4. Заменить часть изображения патчем.
5. Сделать прямой проход по замороженной нейросети (заморожены только веса, градиенты нам нужны).

### Сделай сеть для обучения патча
Тебе нужно:

1) инициализировать обучаемый патч c помощью [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) на изображение, который имеет 3 канала и размеры patch_size на patch_size;

2) реализовать метод `add_patch`, который добавляет патч на изобржение в случайном месте:
  * примени [torch.ranint](https://pytorch.org/docs/stable/generated/torch.randint.html) два раза (или один раз, но сгенерируй два числа) для выбора случайных координат левого нижнего угла на изображении. Патч не должен выходить за пределы изображения;
  * замени части изображений из батча на патч.

#### Твой код
"""

class PatchMobileNet(nn.Module):
  def __init__(self, model, patch_size=32):
    super().__init__()
    # Переводим в режим eval, так как обучать будем только один патч, а не всю сеть
    self.model = model.eval()
    # Создаём матрицу параметров патча
    self.patch = nn.Parameter(Твой код)
    self.patch_size = patch_size
    # Значения для нормализации пригодятся для последующих визуализаций
    self.mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)
    self.std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)

  # Функция, чтобы не выходить за рамки значений для изображения
  def to_img(self, X):
    return X.clamp(0, 1)

  # Нормализация
  def normalize(self, X):
    return (X - self.mean) / self.std

  # Обратная нормализация
  def denormalize(self, X):
    return X * self.std + self.mean

  def add_patch(self, X):
    # Получаем координаты, куда добавляем патч
    x = Твой код
    y = Твой код
    # Меняем часть каждого изображения в батче на патч
    X[Твой код] = self.patch
    return X

  def forward(self, X):
    # Добавляем патч и делаем прямой проход по модели
    patched_img = self.add_patch(X)
    return self.model(patched_img), patched_img.detach() # Возвращаем изображение с патчем для удобства последующей визуализации

"""Теперь инициализируем функции для обучения. Практически всё стандартно, кроме функции потерь. Мы максимизируем вероятность выбранного класса, поэтому и в функции вместо отдельной функции потерь будем передавать номер класса.

#### Твой код
"""

# Функция для одного шага валидации
# От valid step выше единственное отличие — теперь два выхода из модели, потому что модель поменялась. Была нейросеть, которая выдаёт логиты, а теперь к логитам добавилось изображение с патчем
def valid_step(batch, model, label_cls, device):
      X, _ = batch
      X = X.to(device)

      with torch.no_grad():
        logits, _ = model(X) # И для удобства модель будет возвращать кое-что ещё, посмотрим на это чуть ниже. Для валидации нам это не надо
        softmax = F.softmax(logits, dim=1)
        l = # Просто скопируй из valid_step выше
        preds = softmax.argmax(dim=-1)

      return l.item(), preds.cpu().numpy()

# Функция для одного шага обучения
def train_step(batch, model, label_cls, optimizer, device):

    X, _ = batch
    X = X.to(device)

    model.zero_grad()

    logits, _ = model(X)
    l = Твой код # Всё, как и на этапе валидации: Логиты —> Softmax —> Выбрать вероятности, соответствующие нужному классу —> Усреднить

    l.backward()
    optimizer.step()

    return l.item()

# Функция для обучения на эпохе
def train(model, label_cls, optimizer, device, train_dataloader):
    train_loss = 0

    for batch in tqdm(train_dataloader):
      loss_step = train_step(batch, model, label_cls, optimizer, device)
      train_loss += loss_step / len(train_dataloader)

    return train_loss

def train_and_validate(epochs, model, label_cls, optimizer, device, train_dataloader, val_dataloader, save_every=1, naming=''):

    model.to(device)

    for e in range(epochs):

        train_loss = train(model, label_cls, optimizer, device, train_dataloader)
        val_loss, preds = validate(model, label_cls, device, val_dataloader)

        acc = round(accuracy_score(y_test, preds) * 100, 2) # Здесь добавилось вычисление метрик
        acc_cls = round(accuracy_score_cls(preds, label_cls) * 100, 2)

        print(f'Эпоха: {e} | Train Loss {round(train_loss, 4)} | Val Loss {round(val_loss, 4)} | Val acc {acc}% | Val acc cls {acc_cls}%')

"""### 4. Обучи модели с разными патчами и посмотри на результаты (1 балл)

#### Твоё решение
"""

model = PatchMobileNet(model_cls, patch_size=Размер патча, начни с небольшого размера, например 16, но можешь поэксперементировать)
# Параметров очень мало, поэтому можно делать lr побольше
optimizer = Adam([model.patch], maximize=True, lr=Задай темп обучения) # Оптимизируемся в сторону максимума, потому что хотим увеличивать вероятность
epochs = Задай количество эпох

label_cls = 6 # tabby
train_and_validate(epochs, model, label_cls, optimizer, device, train_loader, test_loader, save_every=1, naming='_cat')

"""Обманывать модель получается часто, заменив всего лишь 1/196 пикселей на изображении. Можно немного увеличить патч, и тогда качество возрастёт, или добавить шедулер, чтобы сходимость была стабильнее."""

# Посмотрим на патч
plt.imshow(model.to_img(model.denormalize(model.patch)).cpu().detach().squeeze().permute(1, 2, 0))
plt.show()

"""Опиши, получилась ли у тебя на патче некоторая «котовость».

#### Ответ для ассистента

Опиши полученные визуализации изображений с патчами. Хорошо ли работает патч?

Какая-то структура проглядывается, но выглядит довольно сомнительно, не похоже на кошку. С другой стороны, кажется, кошку не нарисовать сильно лучше на картинке 16 на 16. Тем не менее, возможно, что-то похожее на кошку получилось, потому что чёрные пиксели похожи на очертания головы кошки.

Нарисуем произвольные изображения с патчем и выведем их вероятности принадлежности к классу tabby.
"""

# Можно почитать, что здесь происходит, но это не особо важно. Просто функция для отрисовки изображений
# Немного изменили её по сравнению с началом ДЗ, чтобы дорисовать патчи и напечатать вероятности для класса label_cls. То есть подпись — истинная метка класса, а вероятность — вероятность того, что на картинке находится label_cls
def show_ds_with_probs(ds, n=0): # Принимает датасет и порядковый номер изображения из класса
    # Хотим получить по изображению каждого класса
    fig, axs = plt.subplots(1, len(ds.classes), figsize=(20, 5))
    # Итерируемся по каждому классу
    for cls_num, name in ds.classes.items():
        # Берём n-е изображение класса в датасете
        i = np.argwhere(ds.y == cls_num)[n][0]
        # Рисуем изображение
        ax = axs[cls_num]
        X, _ = ds[i]
        X = X.to(device).unsqueeze(0)
        # Получаем изображение с патчем и вероятности того, что на изображении — кот
        with torch.no_grad():
          logits, patched_X = model(X)
          prob = round(F.softmax(logits, dim=-1)[:, label_cls].cpu().item() * 100, 2)
        X = model.to_img(model.denormalize(patched_X)).squeeze().permute(1, 2, 0).cpu().numpy()
        ax.imshow(X, cmap="Greys_r")
        ax.set_title(f'{name} {prob}%')
        ax.axis("off")
    plt.show()

show_ds_with_probs(train_set, 1)

"""Опиши полученные визуализации изображений с патчами. Хорошо ли работает патч?

#### Ответ для ассистента

Опиши полученные визуализации изображений с патчами. Хорошо ли работает патч?

Патч на изображениях очень маленький, но вероятности, что перед нами на каждой картинке — кот, он явно меняет. Как минимум для картинки с боксёром и многоножки вероятности кота очень большие.

Попробуем с патчем побольше.

#### Твоё решение
"""

model = PatchMobileNet(model_cls, patch_size=Возьми патч побольше)
# Параметров очень мало, поэтому можно делать lr побольше
optimizer = Adam([model.patch], maximize=True, lr=Задай скорость обучения) # Оптимизируемся в сторону максимума, потому что хотим увеличивать вероятность
epochs = Задай количество эпох

label_cls = 6 # tabby
train_and_validate(epochs, model, label_cls, optimizer, device, train_loader, test_loader, save_every=1, naming='_cat')

# Визуализируем полученный патч
plt.imshow(model.to_img(model.denormalize(model.patch)).cpu().detach().squeeze().permute(1, 2, 0))
plt.show()

"""Стал ли патч выглядеть лучше?

#### Ответ для ассистента

Стал ли патч выглядеть лучше?

Разных форм стало больше, возможно, кто-то даже разглядит «котовость» (как будто бы виднеется кот вполоборота), но выглядит довольно сомнительно.

Судя по статьям, для некоторых классов получается выучить патч, который действительно похож на объект соответствующего класса, но далеко не всегда.

В статьях про патчи обычно используют более большие нейросети, изображения с более высоким разрешением и патчи побольше. Возможно, в этом и кроется секрет интерпретируемых с точки зрения человека патчей.
"""

# Можно почитать, что здесь происходит, но это не особо важно. Просто функция для отрисовки изображений
# Немного изменили её по сравнению с началом ДЗ, чтобы дорисовать патчи и напечатать вероятности
def show_ds_with_probs(ds, n=0): # Принимает датасет и порядковый номер изображения из класса
    # Хотим получить по изображению каждого класса
    fig, axs = plt.subplots(1, len(ds.classes), figsize=(20, 5))
    # Итерируемся по каждому классу
    for cls_num, name in ds.classes.items():
        # Берём n-е изображение класса в датасете
        i = np.argwhere(ds.y == cls_num)[n][0]
        # Рисуем изображение
        ax = axs[cls_num]
        X, _ = ds[i]
        X = X.to(device).unsqueeze(0)
        # Получаем изображение с патчем и вероятности для него
        with torch.no_grad():
          logits, patched_X = model(X)
          prob = round(F.softmax(logits, dim=-1)[:, label_cls].cpu().item() * 100, 2)
        X = model.to_img(model.denormalize(patched_X)).squeeze().permute(1, 2, 0).cpu().numpy()
        ax.imshow(X, cmap="Greys_r")
        ax.set_title(f'{name} {prob}%')
        ax.axis("off")
    plt.show()

show_ds_with_probs(train_set, 1)

"""Стало ли лучше?"""